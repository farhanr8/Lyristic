{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vectorize_input.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "f-D8Q7P8En9I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "from PIL import Image, ImageFile\n",
        "import re\n",
        "import nltk\n",
        "import os\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oNXwGv05En9b",
        "colab_type": "code",
        "outputId": "1045f7c1-8f25-4ddb-d5e7-d368c4bad580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F3s5QLmzZzze",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#albums_path = '/Users/Nick/OneDrive/College/2018 Fall/Data Science Lab EE460J/Final Project/output'\n",
        "#output_path = '/Users/Nick/OneDrive/College/2018 Fall/Data Science Lab EE460J/Final Project/embeddings'\n",
        "\n",
        "albums_path = '/content/drive/Team Drives/Audio Vision/Models/output'\n",
        "output_path = '/content/drive/Team Drives/Audio Vision/Models/embeddings'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ef8glHjiEn9s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def format_data(albums_path):\n",
        "    all_lyrics = []\n",
        "    \n",
        "    for dirName, subdirList, fileList in os.walk(albums_path, topdown=False):\n",
        "        for fname in fileList:\n",
        "            \n",
        "            if fname.endswith('.txt'):\n",
        "                lyrics_file = open(dirName + '/' + fname, \"r\")\n",
        "                lyrics = lyrics_file.read()\n",
        "                lyrics_file.close()\n",
        "                \n",
        "                \n",
        "                lyrics = lyrics.replace('\\n', ' ')\n",
        "                lyrics = lyrics.replace('\\\\', '')\n",
        "                lyrics = lyrics.strip(' ')\n",
        "                #lyrics = lyrics.replace(r\"\\[.*\\]\",\"\")\n",
        "                lyrics = re.sub(r'\\[[^\\]]*\\]', '', lyrics)\n",
        "                all_lyrics.append(lyrics.lower())\n",
        "                \n",
        "                \n",
        "            if fname.endswith('.jpg'):\n",
        "                #Hidden file issues in colab\n",
        "                resized_img = Image.open(dirName + '/' + fname).resize((256, 256))\n",
        "                resized_img.save(dirName + '/' + fname)\n",
        "            \n",
        "    return all_lyrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_SjjiXMAEn-o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lyrics = format_data(albums_path)\n",
        "\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(lyrics)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-olqY9X3En_G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 100\n",
        "vec_size = 256\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm=1,\n",
        "                iter=5)\n",
        "  \n",
        "model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "\n",
        "model.save(\"d2v.model\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OddPi-R8En_b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from gensim.models.doc2vec import Doc2Vec\n",
        "\n",
        "# model= Doc2Vec.load(\"d2v.model\")\n",
        "\n",
        "# #to find the vector of a document which is not in training data\n",
        "# test_data = word_tokenize(\"Smells like teen spirit\".lower())\n",
        "# v1 = model.infer_vector(test_data)\n",
        "# print(\"V1_infer\", v1)\n",
        "\n",
        "# # to find most similar doc using tags\n",
        "# similar_doc = model.docvecs.most_similar('1')\n",
        "# print(similar_doc)\n",
        "\n",
        "\n",
        "# # to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
        "# print(model.docvecs['1'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9pmsLZa2En_8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def output_embeddings(output_path, albums_path):\n",
        "    embeddings = {}\n",
        "    \n",
        "    for dirName, subdirList, fileList in os.walk(albums_path, topdown=False):\n",
        "        album_lyrics = ''\n",
        "        album_name = dirName.split('/')[-1]\n",
        "        \n",
        "        for fname in fileList:\n",
        "            \n",
        "            if fname.endswith('.txt'):\n",
        "                lyrics_file = open(dirName + '/' + fname, \"r\")\n",
        "                lyrics = lyrics_file.read()\n",
        "                lyrics_file.close()\n",
        "                \n",
        "                \n",
        "                lyrics = lyrics.replace('\\n', ' ')\n",
        "                lyrics = lyrics.replace('\\\\', '')\n",
        "                lyrics = lyrics.strip(' ')\n",
        "                lyrics = re.sub(r'\\[[^\\]]*\\]', '', lyrics)\n",
        "                album_lyrics = album_lyrics + lyrics.lower() + ' '\n",
        "        \n",
        "        if not len(album_lyrics) == 0:\n",
        "            temp = word_tokenize(album_lyrics)\n",
        "            vector = model.infer_vector(temp)\n",
        "            embeddings[album_name.replace('/','-')] = vector\n",
        "    \n",
        "    return embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oh25XM3ZEoAH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings = output_embeddings(output_path, albums_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CDjEi6fHEoAU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = \"embeddings\"\n",
        "\n",
        "file_object = open(file,'wb')\n",
        "\n",
        "pickle.dump(embeddings, file_object)\n",
        "\n",
        "file_object.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}